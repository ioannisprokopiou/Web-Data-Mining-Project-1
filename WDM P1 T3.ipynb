{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Mini-project I: Twitter & Neo4j\n## Team ID: 3\n\n- Στοϊκοπούλου Ελεονώρα 126\n- Γεροχρήστου Μαργαρίτα 150\n- Προκοπίου Ιωάννης 132",
      "metadata": {},
      "id": "8731953e-7c6d-4811-b1f8-e2391aae690b"
    },
    {
      "cell_type": "code",
      "source": "import json\nfrom bson.json_util import dumps\nfrom pprintpp import pprint\nimport pandas as pd\nfrom typing import NoReturn",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "d62288ae-c5fa-4d07-b0da-ede7636e9f99"
    },
    {
      "cell_type": "code",
      "source": "def create_data_dict() -> dict:\n    \"\"\"Reads a MongoDB json file and returns a python dictionary\n\n    Returns:\n        dict: A dictionary of dictionaries. \n            The dictionaries represent the MongoDB documents.\n    \"\"\"\n    # Open the JSON file from MongoDB (creates a list of json-like strings)\n    with open('WDM1_2.json') as f:\n        data = json.loads(dumps(f))\n\n    # Create a dictionary where each key is the document id\n    data_dict = {}\n\n    for item in data:\n        item_dict = json.loads(item)\n        name = item_dict['_id']['$oid']\n        data_dict[name] = item_dict\n\n    return data_dict",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "ee02641a-c8d3-4c32-8778-01f9dfa95ce2"
    },
    {
      "cell_type": "code",
      "source": "def create_tweets_csvs(data_dict: pd.DataFrame()) -> NoReturn:\n    \"\"\"Create the tweet nodes CSV as well as the tweeted and retweeted relationship CSVs\n\n    tweeted -> user - tweet relationship\n    retweeted -> user - tweet relationship\n\n    Args:\n        data_dict (pd.DataFrame): the data dictionary with all the documents\n    \"\"\"\n    # Create the pandas dataframes\n    tweets = pd.DataFrame(columns=['tweet_id', 'type', 'creation_date'])\n    tweeted = pd.DataFrame(columns=['user_id', 'tweet_id', 'tweet_creation_date'])\n    retweeted = pd.DataFrame(columns=['user_id', 'tweet_id', 'tweet_creation_date'])\n\n    # Loop all the documents in the dictionary\n    for key in data_dict:\n        try:\n            tweet_id = data_dict[key]['includes']['tweets'][0]['id']\n            type = data_dict[key]['data']['referenced_tweets'][0]['type']\n            date = data_dict[key]['includes']['tweets'][0]['created_at_converted']['$date']\n\n            # Add a record in the dataframe for the tweets that\n            # have a 'referenced_tweet'\n\n            tweets.loc[len(tweets)] = [tweet_id, type, date]\n\n            # We check if the case is for a retweet and keep the records for the retweeted relationships\n            if type == 'retweeted':\n                user_id = data_dict[key]['includes']['users'][0]['id']\n                tweet_creation_date = data_dict[key]['data']['created_at']\n\n                retweeted.loc[len(retweeted)] = [user_id, tweet_id, tweet_creation_date]\n\n        except KeyError:\n            # If the field referenced_tweets is not available then we get a KeyError\n            # and it is an original tweet and the type is set to \"tweet\"\n\n            tweet_id = data_dict[key]['includes']['tweets'][0]['id']\n            type = 'tweet'\n            date = data_dict[key]['includes']['tweets'][0]['created_at_converted']['$date']\n\n            tweets.loc[len(tweets)] = [tweet_id, type, date]\n\n            # In this case we have tweets so we keep the records for the tweeted relationships\n            user_id = data_dict[key]['includes']['users'][0]['id']\n            tweet_creation_date = data_dict[key]['data']['created_at']\n            tweeted.loc[len(tweeted)] = [user_id, tweet_id, tweet_creation_date]\n\n\n    tweets['creation_date'] = pd.to_datetime(tweets['creation_date'])\n    tweets['date'] = tweets['creation_date'].dt.date\n\n    # Save the dataframe as CSV\n    tweets.to_csv('tweets.csv', index=False)\n    tweeted.to_csv('tweeted.csv', index=False)\n    retweeted.to_csv('retweeted.csv', index=False)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "4286cba8-9089-4ff6-9cbd-fc596be7d579"
    },
    {
      "cell_type": "code",
      "source": "def create_users_csv(data_dict):\n    \"\"\"Create the user nodes CSV \n\n    Args:\n        data_dict (pd.DataFrame): the data dictionary with all the documents\n    \"\"\"\n    # Create a pandas dataframe\n    users = pd.DataFrame(columns=['user_id', 'following', 'followers', 'tweet_created_at'])\n\n    # Loop all the documents in the dictionary\n    for key in data_dict:\n        try:\n            # Keep all the necessary fields\n            user_id = data_dict[key]['includes']['users'][0]['id']\n            following = data_dict[key]['includes']['users'][0]['public_metrics']['following_count']\n            followers = data_dict[key]['includes']['users'][0]['public_metrics']['followers_count']\n            tweet_created_at = data_dict[key]['data']['created_at']\n\n            # Add record to the dataframe\n            users.loc[len(users)] = [user_id, following,followers, tweet_created_at]\n\n        except KeyError:\n            pass\n\n    # Convert the datetime string to datetime and sort from newest to oldest date\n    users['tweet_created_at'] = pd.to_datetime(users['tweet_created_at'], format=\"%Y-%m-%dT%H:%M:%S.%fZ\")\n    users.sort_values(by='tweet_created_at', ascending=False, inplace=True)\n\n    # Drop duplicate user ids but keep the latest record to have the latest possible following/followers count\n    users.drop_duplicates(subset='user_id', keep='first', inplace=True)\n\n    # Drop tweet created time column\n    users = users.drop(columns='tweet_created_at')\n\n    # Print and save the dataframe as CSV\n    users.to_csv('users.csv', index=False)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "d8d7d8c5-2448-4c44-94d0-a4e923be5d6e"
    },
    {
      "cell_type": "code",
      "source": "def create_url_hashtag_csvs(data_dict):\n    \"\"\"Create the URL and Hashtags nodes CSVs as well as the has_url, used_url,\n    has_hashtag and used_hashtag relationship CSVs\n\n    has_url, has_hashtah -> tweet - entity relationship\n    used_url, used_hashtah -> user - entity relationship\n\n    Args:\n        data_dict (pd.DataFrame): the data dictionary with all the documents\n    \"\"\"\n    # Create the pandas dataframes\n    urls = pd.DataFrame(columns=['url'])\n    has_url = pd.DataFrame(columns=['tweet_id', 'url'])\n    used_url = pd.DataFrame(columns=['user_id', 'url'])\n\n    hashtags = pd.DataFrame(columns=['hashtag'])\n    has_hashtag = pd.DataFrame(columns=['tweet_id', 'hashtag'])\n    used_hashtag = pd.DataFrame(columns=['user_id', 'hashtag'])\n\n    # Loop all the documents in the dictionary\n    for key in data_dict:\n        try:\n            # Keep the entities for the first tweet since it's the one our \n            # document is refering to and we are going to create a record in the tweets.csv\n            \n            tweet_entities = data_dict[key]['includes']['tweets'][0]['entities']\n            \n            # Get all the URLs from the entities of the first tweet\n            if 'urls' in tweet_entities:\n                for url in tweet_entities['urls']:\n\n                    url_str = url['expanded_url']\n\n                    urls.loc[len(urls)] = [url_str]\n                    has_url.loc[len(has_url)] = [data_dict[key]['includes']['tweets'][0]['id'], url_str]\n                    used_url.loc[len(used_url)] = [data_dict[key]['includes']['users'][0]['id'], url_str]\n\n            # Get all the hashtags from the entities of the first tweet\n            if 'hashtags' in tweet_entities:\n                for hashtag in tweet_entities['hashtags']:\n\n                    hashtag_str = hashtag['tag'].str.lower.str.strip()\n\n                    hashtags.loc[len(hashtags)] = [hashtag_str]\n                    has_hashtag.loc[len(has_hashtag)] = [data_dict[key]['includes']['tweets'][0]['id'], hashtag_str]\n                    used_hashtag.loc[len(used_hashtag)] = [data_dict[key]['includes']['users'][0]['id'], hashtag_str]\n        except KeyError:\n            pass\n\n    # Deduplicate the dataframes\n    urls.drop_duplicates(subset=['url'], inplace=True)\n    has_url.drop_duplicates(inplace=True)\n    used_url.drop_duplicates(inplace=True)\n\n    hashtags.drop_duplicates(subset=['hashtag'], inplace=True)\n    has_hashtag.drop_duplicates(inplace=True)\n    used_hashtag.drop_duplicates(inplace=True)\n\n    # Save the dataframes as CSVs\n    urls.to_csv('urls.csv', index=False)\n    has_url.to_csv('has_url.csv', index=False)\n    used_url.to_csv('used_url.csv', index=False)\n\n    hashtags.to_csv('hashtags.csv', index=False)\n    has_hashtag.to_csv('has_hashtag.csv', index=False)\n    used_hashtag.to_csv('used_hashtag.csv', index=False)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "23fa9b74-869a-405d-a129-dc955c50e88d"
    },
    {
      "cell_type": "code",
      "source": "def mentioned_csv(data_dict):\n    \"\"\"Create the mentioned and replied_to relationship CSVs\n\n    mentioned -> user - user relationship\n    replied_to -> user - user relationship\n\n    Args:\n        data_dict (pd.DataFrame): the data dictionary with all the documents\n    \"\"\"\n\n    # Create the pandas dataframes\n    mentioned = pd.DataFrame(columns=['source_user_id', 'target_user_id', 'tweet_id'])\n    replied_to = pd.DataFrame(columns=['source_user_id', 'target_user_id'])\n\n    # Loop all the documents in the dictionary\n    for key in data_dict:\n        try:\n\n            # Extract the source user ID\n            source_user_id = data_dict[key]['includes']['users'][0]['id']\n\n            if 'referenced_tweets' in data_dict[key]['data']:\n                \n                # In the case of retweets we keep the author of tweet[1] \n                # which is the original tweet, and add to the mentioned file since\n                # we want to keep it in the mention-network even if it doesn't exist in the \n                # mentions entities of the tweet\n                if data_dict[key]['data']['referenced_tweets'][0]['type'] == 'retweeted':\n\n                    target_user_id = data_dict[key]['includes']['tweets'][1]['author_id']\n                    mentioned.loc[len(mentioned)] = [source_user_id, target_user_id, data_dict[key]['includes']['tweets'][0]['id']]\n                    continue\n                \n                # In the case of a reply we found out that the original tweet\n                # author could be either under tweets[1]['author_id']\n                # or ['users'][1]['id'] but there were cases that these fields were missing\n                elif data_dict[key]['data']['referenced_tweets'][0]['type'] == 'replied_to':\n                    try:\n                        target_user_id = data_dict[key]['includes']['tweets'][1]['author_id']\n                        replied_to.loc[len(replied_to)] = [source_user_id, target_user_id]\n                    except IndexError:\n                        try:\n                            target_user_id = data_dict[key]['includes']['users'][1]['id']\n                            replied_to.loc[len(replied_to)] = [source_user_id, target_user_id]\n                        except IndexError:\n                            pass\n\n            # Extract mention entities for all the non-retweets \n            for mention in data_dict[key]['includes']['tweets'][0]['entities']['mentions']:\n                mentioned.loc[len(mentioned)] = [source_user_id, mention['id'], data_dict[key]['includes']['tweets'][0]['id']]\n\n        except KeyError:\n            pass\n\n    mentioned.drop_duplicates(inplace=True)\n    replied_to.drop_duplicates(inplace=True)\n\n    # Save the dataframe as CSV\n    mentioned.to_csv('mentioned.csv', index=False)\n    replied_to.to_csv('replied_to.csv', index=False)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "305e25fd-0cab-426e-a704-994e5e2f30b6"
    },
    {
      "cell_type": "code",
      "source": "data_dict = create_data_dict()\ncreate_tweets_csvs(data_dict)\ncreate_users_csv(data_dict)\ncreate_url_hashtag_csvs(data_dict)\nmentioned_csv(data_dict)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "77766e1c-628c-4ecd-8024-89560ebbb9a2"
    }
  ]
}